{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsansao/AprendizadoReforco/blob/main/Licao1_RL_Q_Learning_FrozenLake.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUWdVob0kb_K"
      },
      "source": [
        "# ü§ñ Aprendizado por Refor√ßo: Q-Learning com FrozenLake\n",
        "\n",
        "Este √© um notebook para demonstrar o **Q-Learning Tabular**, um dos algoritmos fundamentais de Aprendizado por Refor√ßo.\n",
        "\n",
        "Vamos treinar um agente para navegar no ambiente \"FrozenLake\" (Lago Congelado) da biblioteca `Gymnasium` (a sucessora do `Gym`).\n",
        "\n",
        "**O Objetivo:**\n",
        "O agente come√ßa no estado 'S' (Start) e precisa chegar ao estado 'G' (Goal/Frisbee), evitando os 'H' (Holes/Buracos).\n",
        "\n",
        "```\n",
        "S F F F\n",
        "F H F H\n",
        "F F F H\n",
        "H F F G\n",
        "```\n",
        "\n",
        "* **S**: Start (In√≠cio)\n",
        "* **F**: Frozen (Congelado - seguro)\n",
        "* **H**: Hole (Buraco - fim de jogo)\n",
        "* **G**: Goal (Objetivo - vit√≥ria)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJlvkD-6kb_M"
      },
      "source": [
        "## Etapa 1: Instalar e Importar as Bibliotecas\n",
        "\n",
        "Precisamos de:\n",
        "* `gymnasium`: Para o ambiente (o jogo).\n",
        "* `numpy`: Para criar e gerenciar nossa Q-Table.\n",
        "* `random`: Para a explora√ß√£o."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZgXzNMNkb_M",
        "outputId": "6a1557df-bf8f-482a-84a6-317dc4885f82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (0.0.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZT7jKw5-kb_N"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import random\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fh6aM29Okb_N"
      },
      "source": [
        "## Etapa 2: Configurar o Ambiente\n",
        "\n",
        "Vamos carregar o ambiente. Usaremos `is_slippery=False` para tornar o problema determin√≠stico por enquanto (se voc√™ mandar ir para 'direita', ele vai para 'direita'). Isso torna o aprendizado mais f√°cil de visualizar.\n",
        "\n",
        "O modo `render_mode='ansi'` nos permite imprimir o jogo no console."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKnTraa9kb_O",
        "outputId": "39bbbf63-26c0-4196-b279-7e8a078328fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---- O Ambiente Inicial ----\n",
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "\n",
            "N√∫mero de estados: 16\n",
            "N√∫mero de a√ß√µes: 4\n",
            "A√ß√µes: 0=Esquerda, 1=Baixo, 2=Direita, 3=Cima\n"
          ]
        }
      ],
      "source": [
        "env = gym.make('FrozenLake-v1', is_slippery=False, render_mode='ansi')\n",
        "\n",
        "# Resetar o ambiente para o estado inicial\n",
        "estado_inicial, info = env.reset()\n",
        "\n",
        "# Mostrar o estado inicial\n",
        "print(\"---- O Ambiente Inicial ----\")\n",
        "print(env.render())\n",
        "\n",
        "# Espa√ßos de A√ß√£o e Estado\n",
        "num_estados = env.observation_space.n\n",
        "num_acoes = env.action_space.n\n",
        "\n",
        "print(f\"N√∫mero de estados: {num_estados}\")\n",
        "print(f\"N√∫mero de a√ß√µes: {num_acoes}\")\n",
        "print(\"A√ß√µes: 0=Esquerda, 1=Baixo, 2=Direita, 3=Cima\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUdqy-Pukb_O"
      },
      "source": [
        "## Etapa 3: Inicializar a Q-Table\n",
        "\n",
        "Aqui est√° o \"c√©rebro\" do nosso agente. √â uma tabela (matriz) com `(n√∫mero de estados) x (n√∫mero de a√ß√µes)`.\n",
        "\n",
        "Vamos inicializar todos os valores Q(s, a) como zero."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2-mc7Z0kb_O",
        "outputId": "86fa77f7-95db-4b1c-d792-f8fa8be30f70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---- Q-Table Inicial (Tudo Zero) ----\n",
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "q_table = np.zeros((num_estados, num_acoes))\n",
        "\n",
        "print(\"---- Q-Table Inicial (Tudo Zero) ----\")\n",
        "print(q_table)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q51hJEk0kb_P"
      },
      "source": [
        "## Etapa 4: Definir os Hiperpar√¢metros\n",
        "\n",
        "Estes s√£o os \"knobs\" que ajustamos para o treinamento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-EYYnfzwkb_P"
      },
      "outputs": [],
      "source": [
        "# Taxa de aprendizado (Alpha)\n",
        "# Qu√£o r√°pido o agente aprende com novas informa√ß√µes.\n",
        "alpha = 0.1\n",
        "\n",
        "# Fator de desconto (Gamma)\n",
        "# O qu√£o importantes s√£o as recompensas futuras (0 = s√≥ importa o agora, 1 = futuro √© t√£o bom quanto).\n",
        "gamma = 0.99\n",
        "\n",
        "# Taxa de explora√ß√£o (Epsilon)\n",
        "# Chance de o agente tomar uma a√ß√£o aleat√≥ria (explorar) vs. a melhor a√ß√£o que ele conhece (explorar).\n",
        "epsilon_inicial = 1.0       # Come√ßa 100% aleat√≥rio\n",
        "epsilon_min = 0.01          # M√≠nimo de 1% aleat√≥rio\n",
        "taxa_decaimento_epsilon = 0.001 # Taxa que o epsilon diminui\n",
        "epsilon = epsilon_inicial\n",
        "\n",
        "# N√∫mero total de epis√≥dios (jogos) que vamos jogar para treinar\n",
        "num_episodios = 20000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0anvlVekb_P"
      },
      "source": [
        "## Etapa 5: O Loop de Treinamento (Q-Learning)\n",
        "\n",
        "Aqui est√° o cora√ß√£o do algoritmo, que segue a **Equa√ß√£o de Bellman**:\n",
        "\n",
        "`Q(s, a) = Q(s, a) + Œ± * [R + Œ≥ * max(Q(s', a')) - Q(s, a)]`\n",
        "\n",
        "Em portugu√™s:\n",
        "`Novo_Valor_Q = Valor_Antigo + Taxa_Aprendizado * [Recompensa + Desconto * Valor_M√°ximo_Futuro - Valor_Antigo]`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6npopBZkb_R",
        "outputId": "a7094c25-0ab8-49e8-bf98-6bbdf593553c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iniciando o treinamento...\n",
            "Epis√≥dio 5000/20000 conclu√≠do. Epsilon: 0.0167\n",
            "Epis√≥dio 10000/20000 conclu√≠do. Epsilon: 0.0100\n",
            "Epis√≥dio 15000/20000 conclu√≠do. Epsilon: 0.0100\n",
            "Epis√≥dio 20000/20000 conclu√≠do. Epsilon: 0.0100\n",
            "Treinamento Conclu√≠do!\n"
          ]
        }
      ],
      "source": [
        "print(\"Iniciando o treinamento...\")\n",
        "\n",
        "for episodio in range(num_episodios):\n",
        "    # 1. Resetar o ambiente para um novo jogo\n",
        "    estado, info = env.reset()\n",
        "    terminado = False\n",
        "    truncado = False\n",
        "\n",
        "    while not terminado and not truncado:\n",
        "        # 2. Decidir A√ß√£o (Explora√ß√£o vs. Explota√ß√£o)\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            acao = env.action_space.sample()  # A√ß√£o aleat√≥ria (Explora√ß√£o)\n",
        "        else:\n",
        "            acao = np.argmax(q_table[estado, :]) # Melhor a√ß√£o conhecida (Explota√ß√£o)\n",
        "\n",
        "        # 3. Tomar a A√ß√£o e observar o resultado\n",
        "        novo_estado, recompensa, terminado, truncado, info = env.step(acao)\n",
        "\n",
        "        # 4. Atualizar a Q-Table (Equa√ß√£o de Bellman)\n",
        "        valor_antigo = q_table[estado, acao]\n",
        "        valor_max_futuro = np.max(q_table[novo_estado, :])\n",
        "\n",
        "        # A f√≥rmula principal do Q-Learning\n",
        "        novo_valor_q = valor_antigo + alpha * (recompensa + gamma * valor_max_futuro - valor_antigo)\n",
        "\n",
        "        q_table[estado, acao] = novo_valor_q\n",
        "\n",
        "        # 5. Atualizar o estado\n",
        "        estado = novo_estado\n",
        "\n",
        "    # Atualizar o Epsilon (Decaimento Exponencial)\n",
        "    # O agente explora menos √† medida que aprende mais\n",
        "    epsilon = epsilon_min + (epsilon_inicial - epsilon_min) * np.exp(-taxa_decaimento_epsilon * episodio)\n",
        "\n",
        "    if (episodio + 1) % 5000 == 0:\n",
        "        print(f\"Epis√≥dio {episodio + 1}/{num_episodios} conclu√≠do. Epsilon: {epsilon:.4f}\")\n",
        "\n",
        "print(\"Treinamento Conclu√≠do!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RddjL2nNkb_R"
      },
      "source": [
        "## Etapa 6: Ver a Q-Table Treinada\n",
        "\n",
        "Vamos ver o que o agente aprendeu. Cada linha √© um estado (0-15), e cada coluna uma a√ß√£o (E, B, D, C).\n",
        "\n",
        "Os valores mais altos indicam a a√ß√£o \"preferida\" do agente em cada estado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i07S9zBZkb_R",
        "outputId": "e058d2a5-3e8d-43cb-d0cd-ff60cb0982c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---- Q-Table Final ----\n",
            "[[0.94148015 0.95099005 0.93206535 0.94148015]\n",
            " [0.94148015 0.         0.79365997 0.86000295]\n",
            " [0.90193369 0.12759019 0.15175136 0.2207541 ]\n",
            " [0.44413131 0.         0.00396355 0.03690548]\n",
            " [0.95099005 0.96059601 0.         0.94148015]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.88202222 0.         0.25169222]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.96059601 0.         0.970299   0.95099005]\n",
            " [0.960596   0.9801     0.9801     0.        ]\n",
            " [0.94687332 0.99       0.         0.67598215]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.98009999 0.99       0.97029898]\n",
            " [0.98009999 0.98999997 1.         0.9801    ]\n",
            " [0.         0.         0.         0.        ]]\n"
          ]
        }
      ],
      "source": [
        "print(\"---- Q-Table Final ----\")\n",
        "print(q_table)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gRwvBUvkb_R"
      },
      "source": [
        "## Etapa 7: Avaliar o Agente Treinado\n",
        "\n",
        "Agora, vamos desligar a explora√ß√£o (`epsilon = 0`) e ver como o agente se sai em 10 jogos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4n86bpikb_R",
        "outputId": "6d39328b-4746-411c-ee6a-ce655e112b56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iniciando testes com o agente treinado...\n",
            "--- Jogo de Teste 1 ---\n",
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "\n",
            "Passo: 1, A√ß√£o: 1\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "\n",
            "Passo: 2, A√ß√£o: 1\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "\n",
            "Passo: 3, A√ß√£o: 2\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "H\u001b[41mF\u001b[0mFG\n",
            "\n",
            "Passo: 4, A√ß√£o: 1\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "\n",
            "Passo: 5, A√ß√£o: 2\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "\n",
            "Passo: 6, A√ß√£o: 2\n",
            "Vit√≥ria! üèÜ\n",
            "--- Jogo de Teste 2 ---\n",
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "\n",
            "Passo: 1, A√ß√£o: 1\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "\n",
            "Passo: 2, A√ß√£o: 1\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "\n",
            "Passo: 3, A√ß√£o: 2\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "H\u001b[41mF\u001b[0mFG\n",
            "\n",
            "Passo: 4, A√ß√£o: 1\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "\n",
            "Passo: 5, A√ß√£o: 2\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "\n",
            "Passo: 6, A√ß√£o: 2\n",
            "Vit√≥ria! üèÜ\n",
            "--- Jogo de Teste 3 ---\n",
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "\n",
            "Passo: 1, A√ß√£o: 1\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "\n",
            "Passo: 2, A√ß√£o: 1\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "\n",
            "Passo: 3, A√ß√£o: 2\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "H\u001b[41mF\u001b[0mFG\n",
            "\n",
            "Passo: 4, A√ß√£o: 1\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "\n",
            "Passo: 5, A√ß√£o: 2\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "\n",
            "Passo: 6, A√ß√£o: 2\n",
            "Vit√≥ria! üèÜ\n",
            "--- Jogo de Teste 4 ---\n",
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "\n",
            "Passo: 1, A√ß√£o: 1\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "\n",
            "Passo: 2, A√ß√£o: 1\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "\n",
            "Passo: 3, A√ß√£o: 2\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "H\u001b[41mF\u001b[0mFG\n",
            "\n",
            "Passo: 4, A√ß√£o: 1\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "\n",
            "Passo: 5, A√ß√£o: 2\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "\n",
            "Passo: 6, A√ß√£o: 2\n",
            "Vit√≥ria! üèÜ\n",
            "--- Jogo de Teste 5 ---\n",
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "\n",
            "Passo: 1, A√ß√£o: 1\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "\n",
            "Passo: 2, A√ß√£o: 1\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "\n",
            "Passo: 3, A√ß√£o: 2\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "H\u001b[41mF\u001b[0mFG\n",
            "\n",
            "Passo: 4, A√ß√£o: 1\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "\n",
            "Passo: 5, A√ß√£o: 2\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "\n",
            "Passo: 6, A√ß√£o: 2\n",
            "Vit√≥ria! üèÜ\n",
            "--- Jogo de Teste 6 ---\n",
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "\n",
            "Passo: 1, A√ß√£o: 1\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "\n",
            "Passo: 2, A√ß√£o: 1\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "\n",
            "Passo: 3, A√ß√£o: 2\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "H\u001b[41mF\u001b[0mFG\n",
            "\n",
            "Passo: 4, A√ß√£o: 1\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "\n",
            "Passo: 5, A√ß√£o: 2\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "\n",
            "Passo: 6, A√ß√£o: 2\n",
            "Vit√≥ria! üèÜ\n",
            "--- Jogo de Teste 7 ---\n",
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "\n",
            "Passo: 1, A√ß√£o: 1\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "\n",
            "Passo: 2, A√ß√£o: 1\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "\n",
            "Passo: 3, A√ß√£o: 2\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "H\u001b[41mF\u001b[0mFG\n",
            "\n",
            "Passo: 4, A√ß√£o: 1\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "\n",
            "Passo: 5, A√ß√£o: 2\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "\n",
            "Passo: 6, A√ß√£o: 2\n",
            "Vit√≥ria! üèÜ\n",
            "--- Jogo de Teste 8 ---\n",
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "\n",
            "Passo: 1, A√ß√£o: 1\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "\n",
            "Passo: 2, A√ß√£o: 1\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "\n",
            "Passo: 3, A√ß√£o: 2\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "H\u001b[41mF\u001b[0mFG\n",
            "\n",
            "Passo: 4, A√ß√£o: 1\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "\n",
            "Passo: 5, A√ß√£o: 2\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "\n",
            "Passo: 6, A√ß√£o: 2\n",
            "Vit√≥ria! üèÜ\n",
            "--- Jogo de Teste 9 ---\n",
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "\n",
            "Passo: 1, A√ß√£o: 1\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "\n",
            "Passo: 2, A√ß√£o: 1\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "\n",
            "Passo: 3, A√ß√£o: 2\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "H\u001b[41mF\u001b[0mFG\n",
            "\n",
            "Passo: 4, A√ß√£o: 1\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "\n",
            "Passo: 5, A√ß√£o: 2\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "\n",
            "Passo: 6, A√ß√£o: 2\n",
            "Vit√≥ria! üèÜ\n",
            "--- Jogo de Teste 10 ---\n",
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "\n",
            "Passo: 1, A√ß√£o: 1\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "\n",
            "Passo: 2, A√ß√£o: 1\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "\n",
            "Passo: 3, A√ß√£o: 2\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "H\u001b[41mF\u001b[0mFG\n",
            "\n",
            "Passo: 4, A√ß√£o: 1\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "\n",
            "Passo: 5, A√ß√£o: 2\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "\n",
            "Passo: 6, A√ß√£o: 2\n",
            "Vit√≥ria! üèÜ\n",
            "\n",
            "Taxa de vit√≥ria em 10 jogos: 100.0%\n"
          ]
        }
      ],
      "source": [
        "num_jogos_teste = 10\n",
        "vitorias = 0\n",
        "frames = [] # Para armazenar os frames da anima√ß√£o\n",
        "\n",
        "print(\"Iniciando testes com o agente treinado...\")\n",
        "\n",
        "for jogo in range(num_jogos_teste):\n",
        "    estado, info = env.reset()\n",
        "    terminado = False\n",
        "    truncado = False\n",
        "    passos = 0\n",
        "    print(f\"--- Jogo de Teste {jogo + 1} ---\")\n",
        "\n",
        "    while not terminado and not truncado:\n",
        "        # Sempre escolher a MELHOR a√ß√£o (sem explora√ß√£o)\n",
        "        acao = np.argmax(q_table[estado, :])\n",
        "\n",
        "        novo_estado, recompensa, terminado, truncado, info = env.step(acao)\n",
        "\n",
        "        # Renderizar e salvar o frame\n",
        "        frame = env.render()\n",
        "        print(frame)\n",
        "        print(f\"Passo: {passos+1}, A√ß√£o: {acao}\")\n",
        "        time.sleep(0.2) # Pausa para podermos assistir\n",
        "\n",
        "        estado = novo_estado\n",
        "        passos += 1\n",
        "\n",
        "        if recompensa == 1.0:\n",
        "            print(\"Vit√≥ria! üèÜ\")\n",
        "            vitorias += 1\n",
        "        elif terminado:\n",
        "            print(\"Caiu no buraco... üï≥Ô∏è\")\n",
        "\n",
        "print(f\"\\nTaxa de vit√≥ria em {num_jogos_teste} jogos: {vitorias / num_jogos_teste * 100}%\")\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrAWFHXTkb_R"
      },
      "source": [
        "## Pr√≥ximos Passos (Desafios)\n",
        "\n",
        "1.  **Tente com `is_slippery=True`:** No modo \"escorregadio\", o agente tem ~33% de chance de ir para uma dire√ß√£o diferente da que ele escolheu. Isso torna o problema muito mais dif√≠cil e realista. Ser√° que o Q-Learning ainda funciona?\n",
        "\n",
        "2.  **Ajuste os Hiperpar√¢metros:** Tente mudar `alpha`, `gamma` e a `taxa_decaimento_epsilon`. Como isso afeta a velocidade e a qualidade do treinamento?\n",
        "\n",
        "3.  **Problemas Maiores:** O Q-Learning Tabular funciona bem para 16 estados. Mas e se tiv√©ssemos 16 *milh√µes* de estados (como no Atari)? A tabela fica grande demais.\n",
        "\n",
        "√â a√≠ que entra o **Deep Q-Network (DQN)** que vimos na aula: substitu√≠mos a *tabela* por uma *Rede Neural*."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}